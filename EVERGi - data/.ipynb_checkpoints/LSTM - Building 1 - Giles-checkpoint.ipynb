{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from copy import deepcopy as dc\n",
    "\n",
    "#from simugrid.misc.scenarios import mordor_microgrid, dieteren_microgrid\n",
    "\n",
    "# Do not modify this class ! To change forecast parameters, go at the end of this script\n",
    "class Forecaster():\n",
    "    '''\n",
    "    Forecaster is based on: https://web.archive.org/web/20200319010116if_/https://www.tensorflow.org/tutorials/structured_data/time_series\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, environment, steps, days):\n",
    "        '''\n",
    "        Initialize data and transorm it in a way for the neural network to understand it\n",
    "        '''\n",
    "        # Intrisic parameters that are available everywhere in this code\n",
    "        self.steps = steps\n",
    "        self.days_to_simulate = days[2]\n",
    "        self.model = None\n",
    "        self.pos = 0\n",
    "        self.train_data = None\n",
    "        self.val_data = None\n",
    "        self.data_mean = 0\n",
    "        self.data_std = 0\n",
    "        self.x_train = []\n",
    "        self.x_test = []\n",
    "        self.y_test = []\n",
    "        self.results = None\n",
    "            \n",
    "        # Import dataset in numpy format\n",
    "        cons = environment['Valeur'].to_numpy()\n",
    "        holidays = environment['working day'].to_numpy()\n",
    "        dates = environment['Datetime']\n",
    "        \n",
    "        self.first_test_day = dates[0] + dt.timedelta(days=days[0]+days[1])\n",
    "        \n",
    "        # Add inputs for neural network\n",
    "        liste=[[],[],[],[]]\n",
    "        for timestep in range(len(dates)): \n",
    "            liste[0].append(dates[timestep].weekday()) \n",
    "            if dates[timestep].weekday() <= 4 : \n",
    "                liste[1].append(0)\n",
    "            else :\n",
    "                liste[1].append(1)\n",
    "            hour = dates[timestep].hour\n",
    "            minu = dates[timestep].minute\n",
    "            liste[2].append(int(hour*4+minu/15))\n",
    "            liste[3].append(dates[timestep].month)\n",
    "        \n",
    "        # Centralize all inputs - PUT THE FORACSTED PARAMETER FIRST IN THE LIST\n",
    "        dataset = np.transpose(np.array([np.array(cons),np.array(holidays),np.array(liste[0]),np.array(liste[1]),np.array(liste[2]),np.array(liste[3])],dtype='float64'))\n",
    "\n",
    "        # Scale values by subtracting the mean and dividing by the standard deviation\n",
    "        self.data_mean = dataset.mean(axis=0)\n",
    "        self.data_std = dataset.std(axis=0)\n",
    "        dataset = (dataset-self.data_mean)/self.data_std      \n",
    "        \n",
    "        if self.steps[1] == 1:\n",
    "            end = len(dataset)\n",
    "        else:\n",
    "            end = None\n",
    "        \n",
    "        def multivariate_data(dataset, pos, start_index, end_index, history_size,\n",
    "                              target_size, step, single_step=False):\n",
    "            \n",
    "            target = dataset[:, pos]\n",
    "            data = []\n",
    "            labels = []\n",
    "            \n",
    "            start_index = start_index + history_size\n",
    "            if end_index is None:\n",
    "                end_index = len(dataset) - target_size \n",
    "            \n",
    "            for i in range(start_index, end_index):\n",
    "                indices = range(i-history_size, i, step)\n",
    "                data.append(dataset[indices])\n",
    "                labels.append(target[i:i+target_size])\n",
    "            \n",
    "            return np.array(data), np.array(labels)\n",
    "\n",
    "\n",
    "        # Split data\n",
    "        self.x_train, y_train = multivariate_data(dataset, self.pos, \n",
    "                                             0, days[0]*self.steps[0], \n",
    "                                             self.steps[0], self.steps[1], self.steps[2])\n",
    "        x_val, y_val = multivariate_data(dataset, self.pos,\n",
    "                                         days[0]*self.steps[0], (days[0] + days[1])*self.steps[0],\n",
    "                                         self.steps[0], self.steps[1], self.steps[2])\n",
    "        self.x_test, self.y_test = multivariate_data(dataset, self.pos,\n",
    "                                                     (days[0] + days[1])*self.steps[0], end, \n",
    "                                                     self.steps[0], self.steps[1], self.steps[2])\n",
    "        \n",
    "        # Special parameters to tune data\n",
    "        BATCH_SIZE = 64 \n",
    "        BUFFER_SIZE = 10000 \n",
    "        \n",
    "        # Shuffle, batch and slice data\n",
    "        tf.keras.backend.set_floatx('float64')\n",
    "        self.train_data = tf.data.Dataset.from_tensor_slices((self.x_train, y_train)).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "        self.val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(BATCH_SIZE).repeat()\n",
    "    \n",
    "        \n",
    "    def training(self, parameters):\n",
    "        '''\n",
    "        The training of the neural network\n",
    "        '''\n",
    "        # Setting seed to ensure reproducibility. \n",
    "        tf.random.set_seed(3)  \n",
    "                \n",
    "        # Special parameters to tune training\n",
    "        STEPS = 100\n",
    "        \n",
    "        # Create model\n",
    "        model = tf.keras.models.Sequential()\n",
    "        \n",
    "        # Create layers\n",
    "        model.add(tf.keras.layers.LSTM(parameters[1], input_shape=self.x_train.shape[-2:]))  # Set number of hidden layers\n",
    "        model.add(tf.keras.layers.Dense(self.steps[1], activation = 'relu'))  # Dense = number of output layers\n",
    "        \n",
    "        # Set objective function\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=parameters[3]), loss = parameters[2])\n",
    "        \n",
    "        # Train\n",
    "        history = model.fit(self.train_data, \n",
    "                            epochs = parameters[0],\n",
    "                            steps_per_epoch = STEPS, \n",
    "                            validation_data = self.val_data,\n",
    "                            validation_steps = STEPS) \n",
    "        # Save model\n",
    "        self.model = model  \n",
    "              \n",
    "        return history\n",
    "    \n",
    "    def save_model(self, name):\n",
    "        '''\n",
    "        Once the neural network is trained, and it is used for several simulations,\n",
    "        it possible to save the training, to avoid re-training every time\n",
    "        '''\n",
    "        self.model.save(name+\".h5\")\n",
    "        \n",
    "    def load_model(self, name):\n",
    "        '''\n",
    "        if model is saved somewhere, and user wants to get it back, he\n",
    "        should use this funtion\n",
    "        '''\n",
    "        self.model = tf.keras.models.load_model(name+\".h5\")\n",
    "    \n",
    "    def plot_train_history(self, history):\n",
    "        '''\n",
    "        Convergence plots to have an idea on how the training performs\n",
    "        '''\n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(range(len(loss)), loss, 'b', label='Training loss')\n",
    "        plt.plot(range(len(loss)), val_loss, 'r', label='Validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Losses')\n",
    "        plt.title('Training and validation losses')\n",
    "        plt.legend()\n",
    "        plt.show()  \n",
    "    \n",
    "    def test_forecaster(self,initial_day):\n",
    "        '''\n",
    "        Only use this function when neural network is trained\n",
    "        '''\n",
    "        true_future = np.array([])\n",
    "        predicted_future = np.array([])\n",
    "        \n",
    "        days_to_test = self.days_to_simulate\n",
    "        steps = self.steps\n",
    "        pos = self.pos\n",
    "        model = self.model\n",
    "        x_test = self.x_test\n",
    "        y_test = self.y_test\n",
    "        data_std = self.data_std\n",
    "        data_mean = self.data_mean\n",
    "        first_day = self.first_test_day\n",
    "        \n",
    "        # off-set between end day of validation set and simulation set\n",
    "        diff = int((initial_day - first_day).total_seconds() / 60 / 15) - steps[0]\n",
    "        \n",
    "        for step in range(int(days_to_test*(steps[0]/steps[1]))):\n",
    "            y = (y_test[diff+step*steps[1]:diff+step*steps[1] + steps[1],pos] * data_std[pos]) + data_mean[pos]  \n",
    "            predict_future = (np.array(model.predict(x_test[diff+step*steps[1]:diff+step*steps[1] + steps[0]])[pos])* data_std[pos]) + data_mean[pos]\n",
    "        \n",
    "            true_future = np.append(true_future, y)\n",
    "            predicted_future = np.append(predicted_future, predict_future)\n",
    "        \n",
    "        self.results = [true_future, predicted_future]\n",
    "    \n",
    "            \n",
    "    def plot_results(self, initial_day):\n",
    "        '''\n",
    "        Once the neural network is tested, this function plots the results\n",
    "        '''\n",
    "        the_real_demand = self.results[0]\n",
    "        the_forecasted_demand = self.results[1]\n",
    "        \n",
    "        x = []\n",
    "        for i in range(len(the_real_demand)):\n",
    "            x.append(initial_day)\n",
    "            initial_day += dt.timedelta(minutes=15)\n",
    "            \n",
    "        plt.figure(figsize=[15,12])\n",
    "        plt.rcParams.update({'font.size':22})\n",
    "        plt.title('Forecast vs real: results')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Demand [kW]')\n",
    "        plt.step(x,the_real_demand, label='Real demand', alpha=0.7)\n",
    "        plt.step(x,the_forecasted_demand, label='Forecasted demand', alpha=0.7)\n",
    "        plt.gcf().autofmt_xdate(rotation=40)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "            \n",
    "        \n",
    "    def validation(self):\n",
    "        '''\n",
    "        Function to print four different validation parameters\n",
    "        '''\n",
    "        the_real_demand = self.results[0]\n",
    "        the_forecasted_demand = self.results[1]\n",
    "        def error_index(forecasted, real, parameter):\n",
    "            ''' \n",
    "            compute some important parameters to compare forecasting results\n",
    "            '''\n",
    "            value = 0\n",
    "            value_1 = 0\n",
    "            value_2 = 0\n",
    "            \n",
    "            if parameter == 'SMAPE':\n",
    "                for i in range(len(forecasted)):\n",
    "                    if real[i] + forecasted[i] == 0:\n",
    "                        value += 0\n",
    "                    else: \n",
    "                        value += ((abs(real[i] - forecasted[i])) / (real[i] + forecasted[i])) * 100\n",
    "                final_value = value / len(forecasted)  \n",
    "                \n",
    "            elif parameter == 'MAPE':\n",
    "                for i in range(len(forecasted)):\n",
    "                    if real[i] == 0:\n",
    "                        value += 0\n",
    "                    else: \n",
    "                        value += (abs(real[i] - forecasted[i]))/real[i]\n",
    "                final_value = value / len(forecasted) * 100\n",
    "                \n",
    "            elif parameter == 'RMSE':\n",
    "                for i in range(len(forecasted)):\n",
    "                    value += (real[i] - forecasted[i]) ** 2\n",
    "                final_value = (value / len(forecasted)) ** (1 / 2) \n",
    "                \n",
    "            elif parameter == 'R':\n",
    "                for i in range(len(forecasted)):\n",
    "                    value += (real[i] - np.mean(real)) * (forecasted[i] - np.mean(forecasted))\n",
    "                    value_1 += (real[i] - np.mean(real)) ** 2\n",
    "                    value_2 += (forecasted[i] - np.mean(forecasted)) ** 2\n",
    "        \n",
    "                if value_1 == 0 or value_2 == 0:\n",
    "                    final_value = 100\n",
    "                else:\n",
    "                    final_value = (value / ((value_1 ** (1 / 2)) * (value_2 ** (1 / 2))))*100\n",
    "                \n",
    "            return final_value\n",
    "\n",
    "        smape_list = []\n",
    "        mape_list = []\n",
    "        rmse_list = []\n",
    "        r_list = []\n",
    "            \n",
    "        for step in range(int(self.days_to_simulate*(self.steps[0]/self.steps[1]))):\n",
    "            begin = step * self.steps[1]\n",
    "            end = begin + self.steps[1]\n",
    "            smape_list.append(error_index(the_forecasted_demand[begin:end], the_real_demand[begin:end], 'SMAPE'))\n",
    "            mape_list.append(error_index(the_forecasted_demand[begin:end], the_real_demand[begin:end], 'MAPE'))\n",
    "            rmse_list.append(error_index(the_forecasted_demand[begin:end], the_real_demand[begin:end], 'RMSE'))\n",
    "            r_list.append(error_index(the_forecasted_demand, the_real_demand, 'R'))\n",
    "        \n",
    "        print('Average smape', np.mean(smape_list))  # Should be as low as possible\n",
    "        print('Average mape', np.mean(mape_list))  # Should be below 5%\n",
    "        print('Average rmse', np.mean(rmse_list))  # Should be as low as possible\n",
    "        print('Average r-correlation', np.mean(r_list))  # Should be close to 100%\n",
    "        \n",
    "    def create_csv(self, days, initial_day):\n",
    "        '''\n",
    "        Receive info to export in csv\n",
    "        For now: Date, time and power\n",
    "        '''       \n",
    "        results = self.results\n",
    "        dates = []\n",
    "        for i in range(len(results[1])):\n",
    "            dates.append(initial_day)\n",
    "            initial_day += dt.timedelta(minutes=15)\n",
    "            \n",
    "        dataset = [dates,results[1]]\n",
    "        \n",
    "        with open('forecasted_demand.csv', 'w') as f:\n",
    "            f.write('Date,\"Forecasted consumption kW\"')\n",
    "            f.write(\"\\n\")\n",
    "            for i in range(len(dataset[0])):\n",
    "                f.write(str(dataset[0][i]) + ',' + str(dataset[1][i]))\n",
    "                f.write(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_learner = Forecaster(\n",
    "    environment = holidata,\n",
    "    steps = [96,48,1],\n",
    "    # days to train, validate and forecast\n",
    "    days = [1100,200,500]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 8s 84ms/step - loss: 0.8450 - val_loss: 1.0431\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 9s 87ms/step - loss: 0.6719 - val_loss: 0.8137\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 9s 86ms/step - loss: 0.5377 - val_loss: 0.7107\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.5092 - val_loss: 0.6706\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.4891 - val_loss: 0.6583\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.4882 - val_loss: 0.6549\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.4970 - val_loss: 0.6328\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.5292 - val_loss: 0.6187\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.5553 - val_loss: 0.6187\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.5291 - val_loss: 0.6078\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.5113 - val_loss: 0.5993\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 0.4985 - val_loss: 0.5898\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 0.4657 - val_loss: 0.5973\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 9s 87ms/step - loss: 0.4967 - val_loss: 0.6031\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.4967 - val_loss: 0.5739\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.4850 - val_loss: 0.5684\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.4380 - val_loss: 0.5624\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.3666 - val_loss: 0.5551\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.3813 - val_loss: 0.5618\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.3947 - val_loss: 0.5637\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.4113 - val_loss: 0.5649\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.4284 - val_loss: 0.5570\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 8s 84ms/step - loss: 0.4437 - val_loss: 0.5575\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.4488 - val_loss: 0.5643\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 0.4757 - val_loss: 0.5580\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 9s 89ms/step - loss: 0.4829 - val_loss: 0.5644\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 10s 100ms/step - loss: 0.4695 - val_loss: 0.5442\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 0.4546 - val_loss: 0.5483\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 0.4217 - val_loss: 0.5731\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 0.4408 - val_loss: 0.5679\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 0.4512 - val_loss: 0.5448\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 10s 95ms/step - loss: 0.4514 - val_loss: 0.5475\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 0.4471 - val_loss: 0.5330\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 0.3788 - val_loss: 0.5370\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 0.3685 - val_loss: 0.5228\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 9s 93ms/step - loss: 0.3571 - val_loss: 0.5317\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 10s 95ms/step - loss: 0.3936 - val_loss: 0.5316\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 9s 91ms/step - loss: 0.4060 - val_loss: 0.5447\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 9s 87ms/step - loss: 0.4044 - val_loss: 0.5426\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.4199 - val_loss: 0.5581\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.4413 - val_loss: 0.5465\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.4678 - val_loss: 0.5447\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.4599 - val_loss: 0.5387\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.4455 - val_loss: 0.5393\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.4257 - val_loss: 0.5476\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 8s 84ms/step - loss: 0.4151 - val_loss: 0.5376\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.4132 - val_loss: 0.5719\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.4283 - val_loss: 0.5360\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.4346 - val_loss: 0.5233\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 9s 86ms/step - loss: 0.4072 - val_loss: 0.5212\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 9s 91ms/step - loss: 0.3459 - val_loss: 0.5178\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.3579 - val_loss: 0.5349\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 0.3443 - val_loss: 0.5436\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 10s 95ms/step - loss: 0.3837 - val_loss: 0.5307\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 9s 86ms/step - loss: 0.4240 - val_loss: 0.5423\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.4043 - val_loss: 0.5402\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.4025 - val_loss: 0.5359\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.4326 - val_loss: 0.5518\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 9s 93ms/step - loss: 0.4458 - val_loss: 0.5515\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 10s 101ms/step - loss: 0.4536 - val_loss: 0.5286\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 0.4268 - val_loss: 0.5118\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 0.4102 - val_loss: 0.5635\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 0.4083 - val_loss: 0.5434\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 0.4185 - val_loss: 0.5447\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 10s 95ms/step - loss: 0.4263 - val_loss: 0.5119\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 0.4187 - val_loss: 0.5124\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 0.3578 - val_loss: 0.5149\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 9s 88ms/step - loss: 0.3394 - val_loss: 0.5421\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 9s 93ms/step - loss: 0.3380 - val_loss: 0.5202\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 0.3586 - val_loss: 0.5185\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 8s 85ms/step - loss: 0.3966 - val_loss: 0.5305\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 9s 90ms/step - loss: 0.3968 - val_loss: 0.5353\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 9s 92ms/step - loss: 0.4020 - val_loss: 0.5208\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.4076 - val_loss: 0.5070\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.4552 - val_loss: 0.5229\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.4352 - val_loss: 0.5193\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.4293 - val_loss: 0.5373\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 8s 85ms/step - loss: 0.4031 - val_loss: 0.5147\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.3912 - val_loss: 0.5310\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 8s 82ms/step - loss: 0.3944 - val_loss: 0.5447\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 9s 93ms/step - loss: 0.4243 - val_loss: 0.5095\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 10s 95ms/step - loss: 0.4160 - val_loss: 0.4968\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 0.3908 - val_loss: 0.5164\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 9s 86ms/step - loss: 0.3362 - val_loss: 0.4939\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 9s 86ms/step - loss: 0.3466 - val_loss: 0.5284\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 0.3479 - val_loss: 0.5222\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.3721 - val_loss: 0.5247\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 8s 76ms/step - loss: 0.4122 - val_loss: 0.5296\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.3896 - val_loss: 0.5127\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.3903 - val_loss: 0.5304\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.4197 - val_loss: 0.5246\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 7s 75ms/step - loss: 0.4223 - val_loss: 0.5324\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.4477 - val_loss: 0.5221\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.4046 - val_loss: 0.5063\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.3908 - val_loss: 0.5403\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 8s 80ms/step - loss: 0.3946 - val_loss: 0.5438\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 8s 81ms/step - loss: 0.3968 - val_loss: 0.5225\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.4127 - val_loss: 0.5093\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 8s 77ms/step - loss: 0.4043 - val_loss: 0.5032\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 8s 83ms/step - loss: 0.3526 - val_loss: 0.5027\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "hidden_layers = 15\n",
    "loss = 'mean_squared_error'  \n",
    "learning_rate = 0.000764\n",
    "# Train model\n",
    "model = deep_learner.training([epochs, hidden_layers, loss, learning_rate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose first day of the forecasting\n",
    "first_day = dt.datetime(year=2018,month=6,day=1,hour=0,minute=0)\n",
    "# The actual forecasting\n",
    "deep_learner.test_forecaster(first_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_learner.plot_results(first_day)\n",
    "# Print score of forecaster (RMSE, etc)\n",
    "deep_learner.validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2017-07-29 02:00:00')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = holidata['Datetime']\n",
    "days = [1100,200,500]\n",
    "first_test_day = dates[0] + dt.timedelta(days=days[0]+days[1])\n",
    "first_test_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
