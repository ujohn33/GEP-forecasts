{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import kerastuner as kt\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input, Model\n",
    "from cond_rnn import ConditionalRNN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "import src.preprocessing\n",
    "from src.functions import load_data, TimeSeriesTensor, create_evaluation_df, series_to_supervised, plot_train_history, validation\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEP1 = pd.read_csv('../data/GEP/Consumption_1H.csv', index_col=0, header=0, names=['value'])\n",
    "GEP4 = pd.read_csv('../data/GEP/B4_Consumption_1H.csv', index_col=0, header=0, names=['value'])\n",
    "datasets = [GEP1, GEP4]\n",
    "names = ['GEP1', 'GEP4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(tf.keras.Model):\n",
    "    def __init__(self, label_index=None):\n",
    "        super().__init__()\n",
    "        self.label_index = label_index\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.label_index is None:\n",
    "            return inputs\n",
    "        result = inputs[:, :, self.label_index]\n",
    "        return result[:, :, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(columns=['mae','mape', 'rmse', 'B'], index=range(28))\n",
    "from progressbar import ProgressBar\n",
    "pbar = ProgressBar()\n",
    "dX_train = []\n",
    "dX_test = []\n",
    "dX_scaler = []\n",
    "for i,df in enumerate(datasets):\n",
    "    train_inputs, test_inputs, y_scaler = MIMO_fulldata_preparation(df, n_test=8760, T=72, HORIZON=72)\n",
    "    baseline = Baseline(label_index = 0)\n",
    "    baseline.compile(loss=tf.losses.MeanSquaredError(), metrics=[tf.metrics.MeanSquaredError()])\n",
    "    predictions_B = baseline.predict(test_inputs['X'])\n",
    "    eval_df_B = create_evaluation_df(predictions_B.reshape(-1,72), test_inputs, 72, y_scaler)\n",
    "    mape = validation(eval_df_B['prediction'], eval_df_B['actual'], 'MAPE')\n",
    "    mae = validation(eval_df_B['prediction'], eval_df_B['actual'], 'MAE')\n",
    "    rmse = validation(eval_df_B['prediction'], eval_df_B['actual'], 'RMSE')\n",
    "    metrics.loc[i] = pd.Series({'mae':mae, 'mape':mape, 'rmse':rmse, 'B': i})\n",
    "metrics.to_csv('./results/GEP/3days/baseline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1356/1356 [==============================] - 42s 31ms/step - loss: 0.0235 - mean_squared_error: 0.0235 - val_loss: 0.0211 - val_mean_squared_error: 0.0211\n",
      "Epoch 2/100\n",
      "1356/1356 [==============================] - 42s 31ms/step - loss: 0.0157 - mean_squared_error: 0.0157 - val_loss: 0.0210 - val_mean_squared_error: 0.0210\n",
      "Epoch 3/100\n",
      "1356/1356 [==============================] - 42s 31ms/step - loss: 0.0144 - mean_squared_error: 0.0144 - val_loss: 0.0202 - val_mean_squared_error: 0.0202\n",
      "Epoch 4/100\n",
      "1356/1356 [==============================] - 42s 31ms/step - loss: 0.0136 - mean_squared_error: 0.0136 - val_loss: 0.0201 - val_mean_squared_error: 0.0201\n",
      "Epoch 5/100\n",
      "1356/1356 [==============================] - 42s 31ms/step - loss: 0.0130 - mean_squared_error: 0.0130 - val_loss: 0.0198 - val_mean_squared_error: 0.0198\n",
      "Epoch 6/100\n",
      "1356/1356 [==============================] - 42s 31ms/step - loss: 0.0125 - mean_squared_error: 0.0125 - val_loss: 0.0194 - val_mean_squared_error: 0.0194\n",
      "Epoch 7/100\n",
      "1356/1356 [==============================] - 42s 31ms/step - loss: 0.0122 - mean_squared_error: 0.0122 - val_loss: 0.0204 - val_mean_squared_error: 0.0204\n",
      "Epoch 8/100\n",
      "1356/1356 [==============================] - 41s 30ms/step - loss: 0.0119 - mean_squared_error: 0.0119 - val_loss: 0.0202 - val_mean_squared_error: 0.0202\n",
      "Epoch 9/100\n",
      "1356/1356 [==============================] - 40s 29ms/step - loss: 0.0116 - mean_squared_error: 0.0116 - val_loss: 0.0201 - val_mean_squared_error: 0.0201\n",
      "Epoch 10/100\n",
      "1356/1356 [==============================] - 39s 29ms/step - loss: 0.0113 - mean_squared_error: 0.0113 - val_loss: 0.0207 - val_mean_squared_error: 0.0207\n",
      "Epoch 11/100\n",
      "1356/1356 [==============================] - 40s 29ms/step - loss: 0.0111 - mean_squared_error: 0.0111 - val_loss: 0.0206 - val_mean_squared_error: 0.0206\n",
      "Epoch 12/100\n",
      "1356/1356 [==============================] - 39s 29ms/step - loss: 0.0109 - mean_squared_error: 0.0109 - val_loss: 0.0204 - val_mean_squared_error: 0.0204\n",
      "Epoch 13/100\n",
      "1356/1356 [==============================] - 40s 29ms/step - loss: 0.0108 - mean_squared_error: 0.0108 - val_loss: 0.0215 - val_mean_squared_error: 0.0215\n",
      "Epoch 14/100\n",
      "1356/1356 [==============================] - 39s 29ms/step - loss: 0.0106 - mean_squared_error: 0.0106 - val_loss: 0.0223 - val_mean_squared_error: 0.0223\n",
      "Epoch 15/100\n",
      "1356/1356 [==============================] - 39s 29ms/step - loss: 0.0104 - mean_squared_error: 0.0104 - val_loss: 0.0219 - val_mean_squared_error: 0.0219\n",
      "Epoch 16/100\n",
      "1356/1356 [==============================] - 39s 29ms/step - loss: 0.0103 - mean_squared_error: 0.0103 - val_loss: 0.0217 - val_mean_squared_error: 0.0217\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCHS = 100\n",
    "BATCHSIZE = 32\n",
    "patience = 10\n",
    "HORIZON = 72\n",
    "\n",
    "class MyTuner(kerastuner.tuners.BayesianOptimization):\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        # You can add additional HyperParameters for preprocessing and custom training loops\n",
    "        # via overriding `run_trial`\n",
    "        kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 1500, 2000, step=100)\n",
    "        super(MyTuner, self).run_trial(trial, *args, **kwargs)# Uses same arguments as the BayesianOptimization Tuner.\n",
    "        \n",
    "class MyHyperModel(HyperModel):\n",
    "    def __init__(self,ins,outs,name):\n",
    "        self.ins  = ins\n",
    "        self.outs = outs\n",
    "        self.name = name\n",
    "\n",
    "    def build(self,hp):\n",
    "        inputs = keras.Input(shape=(self.ins,))\n",
    "        l = hp.Int('layers',1,2)\n",
    "        drop = hp.Float('dropout',min_value=0.1,max_value=0.3,step=0.1)\n",
    "        n = hp.Int('neurons',32,256,step=32)\n",
    "        if l==1:\n",
    "            tf.keras.models.Sequential([\n",
    "                tf.keras.layers.LSTM(32, input_shape=(HORIZON, 15)),\n",
    "                tf.keras.layers.Dense(HORIZON)\n",
    "            ])\n",
    "        elif l==2:\n",
    "            x1 = layers.Dense(n,kernel_initializer='normal',activation='relu')(inputs)\n",
    "            x2 = layers.Dropout(drop)(x1)\n",
    "            x3 = layers.Dense(n,kernel_initializer='normal',activation='relu')(x2)\n",
    "            x4 = layers.Dropout(drop)(x3)\n",
    "            outputs = layers.Dense(self.outs,kernel_initializer='normal',activation='linear')(x4)\n",
    "        # Compile model\n",
    "        model.compile(loss='mse', optimizer='adam',metrics=['mse',mape])\n",
    "        return model\n",
    "\n",
    "tuner = MyTuner(...)\n",
    "# Don't pass epochs or batch_size here, let the Tuner tune them.\n",
    "tuner.search(...)\n",
    "\n",
    "\n",
    "FULL_LSTMIMO = tf.keras.models.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    tf.keras.layers.LSTM(32, input_shape=(HORIZON, 15)),\n",
    "    # Shape => [batch, time, features]\n",
    "    tf.keras.layers.Dense(HORIZON)\n",
    "])\n",
    "\n",
    "\n",
    "metrics = pd.DataFrame(columns=['mae','mape', 'rmse', 'B'], index=range(28))\n",
    "from progressbar import ProgressBar\n",
    "pbar = ProgressBar()\n",
    "dX_train = []\n",
    "dT_train = []\n",
    "dX_test = []\n",
    "dX_scaler = []\n",
    "for i,df in enumerate(datasets):\n",
    "    train_inputs, test_inputs, y_scaler = MIMO_fulldata_preparation(df, n_test=4380, T=72, HORIZON=72)\n",
    "    dX_train.append(train_inputs['X'])\n",
    "    dT_train.append(train_inputs['target'])\n",
    "    dX_test.append(test_inputs)\n",
    "    dX_scaler.append(y_scaler)\n",
    "global_inputs_X = tf.concat(dX_train, 0)\n",
    "global_inputs_T = tf.concat(dT_train, 0)\n",
    "#test_inputs = pd.concat(dn_test, axis=1)\n",
    "\n",
    "# full data LSTM MIMO compilation and fit\n",
    "FULL_LSTMIMO.compile(optimizer=tf.optimizers.Adam(), loss='mse', metrics=[tf.metrics.MeanSquaredError()])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min')\n",
    "        \n",
    "history = FULL_LSTMIMO.fit(global_inputs_X, global_inputs_T, batch_size=32, epochs=MAX_EPOCHS,\n",
    "                      validation_split=0.15,\n",
    "                      callbacks=[early_stopping], verbose=1)\n",
    "for i,df in enumerate(datasets):\n",
    "    FD_predictions = FULL_LSTMIMO.predict(dX_test[i-1]['X'])\n",
    "    FD_eval_df = create_evaluation_df(FD_predictions, dX_test[i-1], HORIZON, dX_scaler[i-1])\n",
    "    mae = validation(FD_eval_df['prediction'], FD_eval_df['actual'], 'MAE')\n",
    "    mape = validation(FD_eval_df['prediction'], FD_eval_df['actual'], 'MAPE')\n",
    "    rmse = validation(FD_eval_df['prediction'], FD_eval_df['actual'], 'RMSE')\n",
    "    #print('rmse {}'.format(rmse))\n",
    "    metrics.loc[i] = pd.Series({'mae':mae, 'mape':mape, 'rmse':rmse, 'B': names[i]})\n",
    "metrics.to_csv('./results/GEP/global/3days/fulldata_LSTM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, n_test):\n",
    "    if len(df) < 8760:\n",
    "        n_test = round(len(df) * 0.2)\n",
    "    test_df = df.copy()[-(n_test+71):]\n",
    "    train_df = df.copy()[:-(len(test_df)-71)]\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConditionalRNN_data_preparation(df, n_test=4380, T=72, HORIZON=72):\n",
    "    df = src.preprocessing.preprocess(df, 'Belgium')\n",
    "    c2 = series_to_supervised(df)\n",
    "    c1 = df.iloc[:,1:9]\n",
    "    c1 = c1.loc[c2.index]\n",
    "    short_df = df.iloc[:,[0,1,-4,-3,-2,-1]].copy()\n",
    "    #check how to formulate more correctly\n",
    "    short_df = short_df[146:]\n",
    "    train_df, test_df = train_test_split(short_df, n_test)\n",
    "    train_c1, test_c1 = train_test_split(c1, n_test)\n",
    "    train_c2, test_c2 = train_test_split(c2, n_test)\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_scaler.fit(train_df[['value']])\n",
    "    train_c2[['value(t-24)']] = y_scaler.fit_transform(train_c2[['value(t-24)']])\n",
    "    train_c2[['value(t-168)']] = y_scaler.fit_transform(train_c2[['value(t-168)']])\n",
    "    X_scaler = MinMaxScaler()\n",
    "    train_df[train_df.columns] = X_scaler.fit_transform(train_df)\n",
    "    test_df[train_df.columns] = X_scaler.fit_transform(test_df)\n",
    "    tensor_structure = {'X':(range(-T+1, 1), ['value','fractional hour_sin','fractional hour_cos','day of year_sin','day of year_cos'])}\n",
    "    train_inputs = TimeSeriesTensor(train_df, 'value', HORIZON, tensor_structure)\n",
    "    test_inputs = TimeSeriesTensor(test_df, 'value', HORIZON, tensor_structure)\n",
    "    train_c1 = c1.reindex(train_inputs.dataframe.index)\n",
    "    train_c2 = c2.reindex(train_inputs.dataframe.index)\n",
    "    test_c1 = c1.reindex(test_inputs.dataframe.index)\n",
    "    test_c2 = c2.reindex(test_inputs.dataframe.index)\n",
    "    return train_inputs, test_inputs, train_c1, test_c1, train_c2, test_c2, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MIMO_fulldata_preparation(df, n_test=4380, T=72, HORIZON=72):\n",
    "    df = src.preprocessing.preprocess(df, 'Belgium')\n",
    "    df = df.merge(series_to_supervised(df), how='right', left_index=True, right_index=True)\n",
    "    train_df, test_df = train_test_split(df, n_test)\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_scaler.fit(train_df[['value']])\n",
    "    long_scaler = MinMaxScaler()\n",
    "    test_df[test_df.columns] = long_scaler.fit_transform(test_df)\n",
    "    train_df[train_df.columns] = long_scaler.fit_transform(train_df)\n",
    "    tensor_structure = {'X':(range(-T+1, 1), train_df.columns)}\n",
    "    train_inputs = TimeSeriesTensor(train_df, 'value', HORIZON, tensor_structure)\n",
    "    test_inputs = TimeSeriesTensor(test_df, 'value', HORIZON, tensor_structure)\n",
    "    return train_inputs, test_inputs, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MIMO_fulldataT_preparation(df, n_test=4380, T=72, HORIZON=72):\n",
    "    df = src.preprocessing.preprocess(df, 'Belgium')\n",
    "    df = df.merge(series_to_supervised(df), how='right', left_index=True, right_index=True)\n",
    "    df = df.merge(temp, how='left', left_index=True, right_index=True)\n",
    "    train_df, test_df = train_test_split(df, n_test)\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_scaler.fit(train_df[['value']])\n",
    "    long_scaler = MinMaxScaler()\n",
    "    test_df[test_df.columns] = long_scaler.fit_transform(test_df)\n",
    "    train_df[train_df.columns] = long_scaler.fit_transform(train_df)\n",
    "    tensor_structure = {'X':(range(-T+1, 1), train_df.columns)}\n",
    "    train_inputs = TimeSeriesTensor(train_df, 'value', HORIZON, tensor_structure)\n",
    "    test_inputs = TimeSeriesTensor(test_df, 'value', HORIZON, tensor_structure)\n",
    "    return train_inputs, test_inputs, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv('../data/GEP/Temp_15min.csv', index_col=0, header=0, names=['temp'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:evgeny] *",
   "language": "python",
   "name": "conda-env-evgeny-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
