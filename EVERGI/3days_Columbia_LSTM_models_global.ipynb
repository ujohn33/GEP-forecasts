{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "organic-aruba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mujohn33\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.login()\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input, Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import src.preprocessing_3days\n",
    "from src.preprocessing_3days import series_to_supervised, preprocess\n",
    "from src.functions import load_data, TimeSeriesTensor, create_evaluation_df, plot_train_history, validation, save_model, load_model\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "pd.options.display.max_seq_items = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "leading-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, n_test):\n",
    "    if len(df) < 8760:\n",
    "        n_test = round(len(df) * 0.2)\n",
    "    test_df = df.copy()[-(n_test+71):]\n",
    "    train_df = df.copy()[:-(len(test_df)-71)]\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "thick-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MIMO_fulldata_preparation(df, n_test=4380, T=72, HORIZON=72):\n",
    "    df = df.merge(series_to_supervised(df), how='right', left_index=True, right_index=True)\n",
    "    df = preprocess(df, 'Canada')\n",
    "    train_df, test_df = train_test_split(df, n_test)\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_scaler.fit(train_df[['value']])\n",
    "    long_scaler = MinMaxScaler()\n",
    "    test_df[test_df.columns] = long_scaler.fit_transform(test_df)\n",
    "    train_df[train_df.columns] = long_scaler.fit_transform(train_df)\n",
    "    #print(train_df.columns)\n",
    "    #tensor_structure = {'X':(range(-T+1, 1), train_df.columns[:1]), 'X2':(range(1, 73), train_df.columns[1:6]), 'static':(None, train_df.columns[6:])}\n",
    "    tensor_structure = {'X':(range(-T+1, 1), train_df.columns[:1]), 'X2':(range(1, 73), train_df.columns[1:])}\n",
    "    #tensor_structure = {'X':(range(-T+1, 1), train_df.columns)}\n",
    "    #print(tensor_structure[0])\n",
    "    train_inputs = TimeSeriesTensor(train_df, 'value', HORIZON, tensor_structure)\n",
    "    test_inputs = TimeSeriesTensor(test_df, 'value', HORIZON, tensor_structure)\n",
    "    return train_inputs, test_inputs, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "hawaiian-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(tf.keras.Model):\n",
    "    def __init__(self, label_index=None):\n",
    "        super().__init__()\n",
    "        self.label_index = label_index\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.label_index is None:\n",
    "            return inputs\n",
    "        result = inputs[:, :, self.label_index]\n",
    "        return result[:, :, tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "adaptive-tutorial",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "metrics = pd.DataFrame(columns=['mae','mape', 'rmse', 'B'], index=range(28))\n",
    "from progressbar import ProgressBar\n",
    "pbar = ProgressBar()\n",
    "dX_train = []\n",
    "dX_test = []\n",
    "dX_scaler = []\n",
    "for i in pbar(range(1,28)):\n",
    "    filename = '../data/Columbia_clean/Residential_'+str(i)+'.csv'\n",
    "    df = pd.read_csv(filename, index_col=0)\n",
    "    train_inputs, test_inputs, y_scaler = MIMO_fulldata_preparation(df)\n",
    "    baseline = Baseline(label_index = 0)\n",
    "    baseline.compile(loss=tf.losses.MeanSquaredError(), metrics=[tf.metrics.MeanSquaredError()])\n",
    "    predictions_B = baseline.predict(test_inputs['X'])\n",
    "    eval_df_B = create_evaluation_df(predictions_B.reshape(-1,24), test_inputs, 24, y_scaler)\n",
    "    mape = validation(eval_df_B['prediction'], eval_df_B['actual'], 'MAPE')\n",
    "    mae = validation(eval_df_B['prediction'], eval_df_B['actual'], 'MAE')\n",
    "    rmse = validation(eval_df_B['prediction'], eval_df_B['actual'], 'RMSE')\n",
    "    metrics.loc[i-1] = pd.Series({'mae':mae, 'mape':mape, 'rmse':rmse, 'B': i})\n",
    "metrics.to_csv('./results/Columbia/global/baseline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-filing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3bhbbzbg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 940208<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Problem finishing run\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/evgeny/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 1400, in _atexit_cleanup\n",
      "    self._on_finish()\n",
      "  File \"/home/ubuntu/anaconda3/envs/evgeny/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 1558, in _on_finish\n",
      "    self._backend.interface.publish_telemetry(self._telemetry_obj)\n",
      "  File \"/home/ubuntu/anaconda3/envs/evgeny/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\", line 225, in publish_telemetry\n",
      "    self._publish(rec)\n",
      "  File \"/home/ubuntu/anaconda3/envs/evgeny/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\", line 514, in _publish\n",
      "    raise Exception(\"The wandb backend process has shutdown\")\n",
      "Exception: The wandb backend process has shutdown\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:3bhbbzbg). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.25 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.20<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">fancy-lake-6</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/ujohn33/3days_forcast\" target=\"_blank\">https://wandb.ai/ujohn33/3days_forcast</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/ujohn33/3days_forcast/runs/sczq4rj1\" target=\"_blank\">https://wandb.ai/ujohn33/3days_forcast/runs/sczq4rj1</a><br/>\n",
       "                Run data is saved locally in <code>/home/ubuntu/evgeny/GEP-forecasts/EVERGI/wandb/run-20210409_162746-sczq4rj1</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "281/281 [==============================] - 73s 259ms/step - loss: 0.0111 - mean_squared_error: 0.0111 - val_loss: 0.0108 - val_mean_squared_error: 0.0108\n",
      "Epoch 2/100\n",
      "281/281 [==============================] - 73s 258ms/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0104 - val_mean_squared_error: 0.0104\n",
      "Epoch 3/100\n",
      "281/281 [==============================] - 72s 258ms/step - loss: 0.0077 - mean_squared_error: 0.0077 - val_loss: 0.0102 - val_mean_squared_error: 0.0102\n",
      "Epoch 4/100\n",
      "281/281 [==============================] - 73s 259ms/step - loss: 0.0074 - mean_squared_error: 0.0074 - val_loss: 0.0101 - val_mean_squared_error: 0.0101\n",
      "Epoch 5/100\n",
      "281/281 [==============================] - 71s 253ms/step - loss: 0.0071 - mean_squared_error: 0.0071 - val_loss: 0.0100 - val_mean_squared_error: 0.0100\n",
      "Epoch 6/100\n",
      "281/281 [==============================] - 72s 255ms/step - loss: 0.0069 - mean_squared_error: 0.0069 - val_loss: 0.0098 - val_mean_squared_error: 0.0098\n",
      "Epoch 7/100\n",
      "281/281 [==============================] - 72s 256ms/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0098 - val_mean_squared_error: 0.0098\n",
      "Epoch 8/100\n",
      "281/281 [==============================] - 72s 255ms/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0097 - val_mean_squared_error: 0.0097\n",
      "Epoch 9/100\n",
      "281/281 [==============================] - 72s 255ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - val_loss: 0.0097 - val_mean_squared_error: 0.0097\n",
      "Epoch 10/100\n",
      "281/281 [==============================] - 71s 254ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - val_loss: 0.0097 - val_mean_squared_error: 0.0097\n",
      "Epoch 11/100\n",
      "281/281 [==============================] - 72s 255ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - val_loss: 0.0097 - val_mean_squared_error: 0.0097\n",
      "Epoch 12/100\n",
      "281/281 [==============================] - 72s 256ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - val_loss: 0.0097 - val_mean_squared_error: 0.0097\n",
      "Epoch 13/100\n",
      "281/281 [==============================] - ETA: 0s - loss: 0.0066 - mean_squared_error: 0.0066"
     ]
    }
   ],
   "source": [
    "MAX_EPOCHS = 100\n",
    "BATCHSIZE = 1500\n",
    "patience = 100\n",
    "HORIZON = 72\n",
    "\n",
    "\n",
    "FULL_LSTMIMO = tf.keras.models.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    tf.keras.layers.LSTM(32, input_shape=(HORIZON, 14)),\n",
    "    # Shape => [batch, time, features]\n",
    "    tf.keras.layers.Dense(HORIZON)\n",
    "])\n",
    "\n",
    "metrics = pd.DataFrame(columns=['mae','mape', 'rmse', 'B'], index=range(28))\n",
    "dX_train = []\n",
    "dT_train = []\n",
    "dX_test = []\n",
    "dX_scaler = []\n",
    "for i in range(1,29):\n",
    "    filename = '../data/Columbia_clean/Residential_'+str(i)+'.csv'\n",
    "    df = pd.read_csv(filename, index_col=0)\n",
    "    train_inputs, test_inputs, y_scaler = MIMO_fulldata_preparation(df, n_test=4380, T=72, HORIZON=72)\n",
    "    dX_train.append(tf.concat([train_inputs['X'],train_inputs['X2']], axis=2))\n",
    "    dT_train.append(train_inputs['target'])\n",
    "    dX_test.append(test_inputs)\n",
    "    dX_scaler.append(y_scaler)\n",
    "global_inputs_X = tf.concat(dX_train, 0)\n",
    "global_inputs_T = tf.concat(dT_train, 0)\n",
    "#test_inputs = pd.concat(dn_test, axis=1)\n",
    "\n",
    "# 1️⃣ Start a new run, tracking config metadata\n",
    "run = wandb.init(project=\"3days_forcast\", config={\n",
    "    \"batch_size\": BATCHSIZE,\n",
    "    \"architecture\": \"RNN with forward lags for temporal\",\n",
    "    \"dataset\": \"Columbia\",\n",
    "    \"epochs\": MAX_EPOCHS,\n",
    "    'patience': patience \n",
    "})\n",
    "config = wandb.config\n",
    "\n",
    "# full data LSTM MIMO compilation and fit\n",
    "FULL_LSTMIMO.compile(optimizer=tf.optimizers.Adam(), loss='mse', metrics=[tf.metrics.MeanSquaredError()])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min')\n",
    "        \n",
    "history = FULL_LSTMIMO.fit(global_inputs_X, global_inputs_T, batch_size=BATCHSIZE, epochs=MAX_EPOCHS,\n",
    "                      validation_split=0.15,\n",
    "                      callbacks=[early_stopping, WandbCallback()], verbose=1)\n",
    "save_model(FULL_LSTMIMO, 'Columbia_model')\n",
    "\n",
    "for i in tqdm(range(0,28)):\n",
    "    concat_input = tf.concat([dX_test[i]['X'],dX_test[i]['X2']], axis=2)\n",
    "    FD_predictions = FULL_LSTMIMO.predict(concat_input)\n",
    "    FD_eval_df = create_evaluation_df(FD_predictions, dX_test[i], HORIZON, dX_scaler[i])\n",
    "    mae = validation(FD_eval_df['prediction'], FD_eval_df['actual'], 'MAE')\n",
    "    mape = validation(FD_eval_df['prediction'], FD_eval_df['actual'], 'MAPE')\n",
    "    rmse = validation(FD_eval_df['prediction'], FD_eval_df['actual'], 'RMSE')\n",
    "    #print('rmse {}'.format(rmse))\n",
    "    metrics.loc[i] = pd.Series({'mae':mae, 'mape':mape, 'rmse':rmse, 'B': i})\n",
    "wandb.log({\"mape\": metrics.mape.mean()})\n",
    "wandb.log({\"rmse\": metrics.rmse.mean()})\n",
    "wandb.log({\"mae\": metrics.mae.mean()})\n",
    "run.finish()\n",
    "metrics.to_csv('./results/Columbia/global/3days/revised2_LSTM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 100\n",
    "BATCHSIZE = 1500\n",
    "patience = 100\n",
    "HORIZON = 72\n",
    "\n",
    "\n",
    "FULL_LSTMIMO = tf.keras.models.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    tf.keras.layers.LSTM(32, input_shape=(HORIZON, 14), return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.LSTM(32),\n",
    "    # Shape => [batch, time, features]\n",
    "    tf.keras.layers.Dense(HORIZON)\n",
    "])\n",
    "\n",
    "\n",
    "metrics = pd.DataFrame(columns=['mae','mape', 'rmse', 'B'], index=range(28))\n",
    "dX_train = []\n",
    "dT_train = []\n",
    "dX_test = []\n",
    "dX_scaler = []\n",
    "for i in range(1,29):\n",
    "    filename = '../data/Columbia_clean/Residential_'+str(i)+'.csv'\n",
    "    df = pd.read_csv(filename, index_col=0)\n",
    "    train_inputs, test_inputs, y_scaler = MIMO_fulldata_preparation(df, n_test=4380, T=72, HORIZON=72)\n",
    "    dX_train.append(tf.concat([train_inputs['X'],train_inputs['X2']], axis=2))\n",
    "    dT_train.append(train_inputs['target'])\n",
    "    dX_test.append(test_inputs)\n",
    "    dX_scaler.append(y_scaler)\n",
    "global_inputs_X = tf.concat(dX_train, 0)\n",
    "global_inputs_T = tf.concat(dT_train, 0)\n",
    "#test_inputs = pd.concat(dn_test, axis=1)\n",
    "\n",
    "# 1️⃣ Start a new run, tracking config metadata\n",
    "run = wandb.init(project=\"3days_forcast\", config={\n",
    "    \"batch_size\": BATCHSIZE,\n",
    "    \"architecture\": \"RNN 2 layers\",\n",
    "    \"dataset\": \"Columbia\",\n",
    "    \"epochs\": MAX_EPOCHS,\n",
    "    'patience': patience \n",
    "})\n",
    "config = wandb.config\n",
    "\n",
    "# full data LSTM MIMO compilation and fit\n",
    "FULL_LSTMIMO.compile(optimizer=tf.optimizers.Adam(), loss='mse', metrics=[tf.metrics.MeanSquaredError()])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min')\n",
    "        \n",
    "history = FULL_LSTMIMO.fit(global_inputs_X, global_inputs_T, batch_size=BATCHSIZE, epochs=MAX_EPOCHS,\n",
    "                      validation_split=0.15,\n",
    "                      callbacks=[early_stopping, WandbCallback()], verbose=1)\n",
    "save_model(FULL_LSTMIMO, 'Columbia_model')\n",
    "\n",
    "for i in tqdm(range(0,28)):\n",
    "    concat_input = tf.concat([dX_test[i]['X'],dX_test[i]['X2']], axis=2)\n",
    "    FD_predictions = FULL_LSTMIMO.predict(concat_input)\n",
    "    FD_eval_df = create_evaluation_df(FD_predictions, dX_test[i], HORIZON, dX_scaler[i])\n",
    "    mae = validation(FD_eval_df['prediction'], FD_eval_df['actual'], 'MAE')\n",
    "    mape = validation(FD_eval_df['prediction'], FD_eval_df['actual'], 'MAPE')\n",
    "    rmse = validation(FD_eval_df['prediction'], FD_eval_df['actual'], 'RMSE')\n",
    "    #print('rmse {}'.format(rmse))\n",
    "    metrics.loc[i] = pd.Series({'mae':mae, 'mape':mape, 'rmse':rmse, 'B': i})\n",
    "wandb.log({\"mape\": metrics.mape.mean()})\n",
    "wandb.log({\"rmse\": metrics.rmse.mean()})\n",
    "wandb.log({\"mae\": metrics.mae.mean()})\n",
    "run.finish()\n",
    "metrics.to_csv('./results/Columbia/global/3days/LSTM_2layers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "lightweight-quilt",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'tuple'> input: (<tf.Tensor 'IteratorGetNext:0' shape=(None, 24, 5) dtype=float64>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 8) dtype=float64>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 2) dtype=float64>)\n",
      "Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layer dense_16 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_17 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer rnn_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'tuple'> input: (<tf.Tensor 'IteratorGetNext:0' shape=(None, 24, 5) dtype=float64>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 8) dtype=float64>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 2) dtype=float64>)\n",
      "Consider rewriting this model with the Functional API.\n",
      "12675/12676 [============================>.] - ETA: 0s - loss: 0.0070 - mean_squared_error: 0.0070WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'tuple'> input: (<tf.Tensor 'IteratorGetNext:0' shape=(None, 24, 5) dtype=float64>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 8) dtype=float64>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 2) dtype=float64>)\n",
      "Consider rewriting this model with the Functional API.\n",
      "12676/12676 [==============================] - 286s 23ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 2/100\n",
      "12676/12676 [==============================] - 287s 23ms/step - loss: 0.0066 - mean_squared_error: 0.0066 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "Epoch 3/100\n",
      "12676/12676 [==============================] - 287s 23ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "Epoch 4/100\n",
      "12676/12676 [==============================] - 282s 22ms/step - loss: 0.0065 - mean_squared_error: 0.0065 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 5/100\n",
      "12676/12676 [==============================] - 286s 23ms/step - loss: 0.0064 - mean_squared_error: 0.0064 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 6/100\n",
      "12676/12676 [==============================] - 289s 23ms/step - loss: 0.0064 - mean_squared_error: 0.0064 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "Epoch 7/100\n",
      "12676/12676 [==============================] - 288s 23ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 8/100\n",
      "12676/12676 [==============================] - 269s 21ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 9/100\n",
      "12676/12676 [==============================] - 164s 13ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 10/100\n",
      "12676/12676 [==============================] - 168s 13ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "Epoch 11/100\n",
      "12676/12676 [==============================] - 161s 13ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "Epoch 12/100\n",
      "12676/12676 [==============================] - 161s 13ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 13/100\n",
      "12676/12676 [==============================] - 160s 13ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0085 - val_mean_squared_error: 0.0085\n",
      "Epoch 14/100\n",
      "12676/12676 [==============================] - 161s 13ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 15/100\n",
      "12676/12676 [==============================] - 161s 13ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 16/100\n",
      "12676/12676 [==============================] - 161s 13ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 17/100\n",
      "12676/12676 [==============================] - 161s 13ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 18/100\n",
      "12676/12676 [==============================] - 161s 13ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 19/100\n",
      "12676/12676 [==============================] - 161s 13ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 20/100\n",
      "12676/12676 [==============================] - 162s 13ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 21/100\n",
      "12676/12676 [==============================] - 162s 13ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "Epoch 22/100\n",
      "12676/12676 [==============================] - 162s 13ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "Epoch 23/100\n",
      "12676/12676 [==============================] - 162s 13ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor, but we receive a <class 'tuple'> input: (<tf.Tensor 'IteratorGetNext:0' shape=(None, 24, 5) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 8) dtype=float64>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 2) dtype=float64>)\n",
      "Consider rewriting this model with the Functional API.\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCHS = 100\n",
    "BATCHSIZE = 32\n",
    "patience = 10\n",
    "HORIZON = 24\n",
    "\n",
    "#ConditionalRNN = tf.keras.models.Sequential([\n",
    "#    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "#    tf.keras.layers.LSTM(32, input_shape=(24, 15)),\n",
    "#    # Shape => [batch, time, features]\n",
    "#    tf.keras.layers.Dense(HORIZON)\n",
    "#])\n",
    "\n",
    "ConditionalRNN = Sequential(layers=[ConditionalRNN(32, cell='LSTM'),\n",
    "                                    Dense(HORIZON)])\n",
    "\n",
    "\n",
    "metrics = pd.DataFrame(columns=['mae','mape', 'rmse', 'B'], index=range(28))\n",
    "from progressbar import ProgressBar\n",
    "pbar = ProgressBar()\n",
    "dX_train = []\n",
    "dc1_train = []\n",
    "dc2_train = []\n",
    "dT_train = []\n",
    "dX_test = []\n",
    "dc1_test = []\n",
    "dc2_test = []\n",
    "dX_scaler = []\n",
    "for i in pbar(range(1,28)):\n",
    "    filename = '../data/Columbia_clean/Residential_'+str(i)+'.csv'\n",
    "    df = pd.read_csv(filename, index_col=0)\n",
    "    train_inputs, test_inputs, train_c1, test_c1, train_c2, test_c2, y_scaler = ConditionalRNN_data_preparation(df)\n",
    "    dX_train.append(train_inputs['X'])\n",
    "    dT_train.append(train_inputs['target'])\n",
    "    dc1_train.append(train_c1)\n",
    "    dc2_train.append(train_c2)\n",
    "    dc1_test.append(test_c1)\n",
    "    dc2_test.append(test_c2)\n",
    "    dX_test.append(test_inputs)\n",
    "    dX_scaler.append(y_scaler)\n",
    "global_inputs_X = tf.concat(dX_train, 0)\n",
    "global_inputs_c1 = tf.concat(dc1_train, 0)\n",
    "global_inputs_c2 = tf.concat(dc2_train, 0)\n",
    "global_inputs_T = tf.concat(dT_train, 0)\n",
    "#test_inputs = pd.concat(dn_test, axis=1)\n",
    "\n",
    "# full data LSTM MIMO compilation and fit\n",
    "ConditionalRNN.compile(optimizer=tf.optimizers.Adam(), loss='mse', metrics=[tf.metrics.MeanSquaredError()])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min')\n",
    "        \n",
    "history = ConditionalRNN.fit([global_inputs_X, global_inputs_c1, global_inputs_c2], global_inputs_T, batch_size=32, epochs=MAX_EPOCHS,\n",
    "                      validation_split=0.15,\n",
    "                      callbacks=[early_stopping], verbose=1)\n",
    "for i in range(1,28):\n",
    "    FD_predictions = ConditionalRNN.predict([dX_test[i-1]['X'],dc1_test[i-1],dc2_test[i-1]])\n",
    "    FD_eval_df = create_evaluation_df(FD_predictions, dX_test[i-1], HORIZON, dX_scaler[i-1])\n",
    "    mae = validation(FD_eval_df['prediction'], FD_eval_df['actual'], 'MAE')\n",
    "    mape = validation(FD_eval_df['prediction'], FD_eval_df['actual'], 'MAPE')\n",
    "    rmse = validation(FD_eval_df['prediction'], FD_eval_df['actual'], 'RMSE')\n",
    "    #print('rmse {}'.format(rmse))\n",
    "    metrics.loc[i-1] = pd.Series({'mae':mae, 'mape':mape, 'rmse':rmse, 'B': i})\n",
    "metrics.to_csv('./results/Columbia/global/conditional_LSTM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "racial-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConditionalRNN_data_preparation(df, n_test=4380, T=24, HORIZON=24):\n",
    "    df = src.preprocessing.preprocess(df, 'Canada')\n",
    "    c2 = series_to_supervised(df)\n",
    "    c1 = df.iloc[:,1:9]\n",
    "    c1 = c1.loc[c2.index]\n",
    "    short_df = df.iloc[:,[0,1,-4,-3,-2,-1]].copy()\n",
    "    #check how to formulate more correctly\n",
    "    short_df = short_df[146:]\n",
    "    train_df, test_df = train_test_split(short_df, n_test)\n",
    "    train_c1, test_c1 = train_test_split(c1, n_test)\n",
    "    train_c2, test_c2 = train_test_split(c2, n_test)\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_scaler.fit(train_df[['value']])\n",
    "    train_c2[['value(t-24)']] = y_scaler.fit_transform(train_c2[['value(t-24)']])\n",
    "    train_c2[['value(t-168)']] = y_scaler.fit_transform(train_c2[['value(t-168)']])\n",
    "    X_scaler = MinMaxScaler()\n",
    "    train_df[train_df.columns] = X_scaler.fit_transform(train_df)\n",
    "    test_df[train_df.columns] = X_scaler.fit_transform(test_df)\n",
    "    tensor_structure = {'X':(range(-T+1, 1), ['value','fractional hour_sin','fractional hour_cos','day of year_sin','day of year_cos'])}\n",
    "    train_inputs = TimeSeriesTensor(train_df, 'value', HORIZON, tensor_structure)\n",
    "    test_inputs = TimeSeriesTensor(test_df, 'value', HORIZON, tensor_structure)\n",
    "    train_c1 = c1.reindex(train_inputs.dataframe.index)\n",
    "    train_c2 = c2.reindex(train_inputs.dataframe.index)\n",
    "    test_c1 = c1.reindex(test_inputs.dataframe.index)\n",
    "    test_c2 = c2.reindex(test_inputs.dataframe.index)\n",
    "    return train_inputs, test_inputs, train_c1, test_c1, train_c2, test_c2, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-gathering",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-belly",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-teacher",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
