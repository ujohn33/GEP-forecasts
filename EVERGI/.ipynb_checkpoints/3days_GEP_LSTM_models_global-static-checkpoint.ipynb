{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "recreational-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input, Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import src.preprocessing_3days\n",
    "from src.preprocessing_3days import series_to_supervised, preprocess\n",
    "from src.functions import load_data, TimeSeriesTensor, create_evaluation_df, plot_train_history, validation\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "pd.options.display.max_seq_items = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ahead-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, n_test):\n",
    "    if len(df) < 8760:\n",
    "        n_test = round(len(df) * 0.2)\n",
    "    test_df = df.copy()[-(n_test+71):]\n",
    "    train_df = df.copy()[:-(len(test_df)-71)]\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "parallel-fever",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MIMO_fulldata_preparation(df, n_test=4380, T=72, HORIZON=72):\n",
    "    df = preprocess(df, 'Belgium')\n",
    "    df = df.merge(series_to_supervised(df), how='right', left_index=True, right_index=True)\n",
    "    train_df, test_df = train_test_split(df, n_test)\n",
    "    y_scaler = MinMaxScaler()\n",
    "    y_scaler.fit(train_df[['value']])\n",
    "    long_scaler = MinMaxScaler()\n",
    "    test_df[test_df.columns] = long_scaler.fit_transform(test_df)\n",
    "    train_df[train_df.columns] = long_scaler.fit_transform(train_df)\n",
    "    #print(train_df.columns)\n",
    "    tensor_structure = {'X':(range(-T+1, 1), train_df.columns[:5]), 'X2':(range(1, 73), train_df.columns[-1:]), 'static':(None, train_df.columns[5:-1])}\n",
    "    #tensor_structure = {'X':(range(-24+1, 1), train_df.columns[:1]), 'X2':(range(-168+1, -96+1), train_df.columns[:1]), 'static':(None, train_df.columns[3:])}\n",
    "    #tensor_structure = {'X':(range(-T+1, 1), train_df.columns)}\n",
    "    #print(tensor_structure[0])\n",
    "    train_inputs = TimeSeriesTensor(train_df, 'value', HORIZON, tensor_structure)\n",
    "    test_inputs = TimeSeriesTensor(test_df, 'value', HORIZON, tensor_structure)\n",
    "    return train_inputs, test_inputs, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "stylish-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEP1 = pd.read_csv('../data/GEP/Consumption_1H.csv', index_col=0, header=0, names=['value'])\n",
    "GEP4 = pd.read_csv('../data/GEP/B4_Consumption_1H.csv', index_col=0, header=0, names=['value'])\n",
    "datasets = [GEP1, GEP4]\n",
    "names = ['GEP1', 'GEP4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "proved-overall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(-71, 1)\n",
      "Index(['value', 'fractional hour_sin', 'fractional hour_cos',\n",
      "       'day of year_sin', 'day of year_cos'],\n",
      "      dtype='object')\n",
      "range(1, 73)\n",
      "Index(['value(t-168)'], dtype='object')\n",
      "None\n",
      "Index(['working day', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5',\n",
      "       'week_6', 'week_7'],\n",
      "      dtype='object')\n",
      "range(-71, 1)\n",
      "Index(['value', 'fractional hour_sin', 'fractional hour_cos',\n",
      "       'day of year_sin', 'day of year_cos'],\n",
      "      dtype='object')\n",
      "range(1, 73)\n",
      "Index(['value(t-168)'], dtype='object')\n",
      "None\n",
      "Index(['working day', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5',\n",
      "       'week_6', 'week_7'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_inputs, test_inputs, y_scaler = MIMO_fulldata_preparation(GEP4, n_test=4380, T=72, HORIZON=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "mounted-trading",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11293, 8)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs['static'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "original-thickness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11293, 72, 5)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "reasonable-senator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11293, 72, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs['X2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "noted-traveler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(-71, 1)\n",
      "Index(['value'], dtype='object')\n",
      "None\n",
      "Index(['working day', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5',\n",
      "       'week_6', 'week_7', 'fractional hour_sin', 'fractional hour_cos',\n",
      "       'day of year_sin', 'day of year_cos', 'value(t-168)'],\n",
      "      dtype='object')\n",
      "range(-71, 1)\n",
      "Index(['value'], dtype='object')\n",
      "None\n",
      "Index(['working day', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5',\n",
      "       'week_6', 'week_7', 'fractional hour_sin', 'fractional hour_cos',\n",
      "       'day of year_sin', 'day of year_cos', 'value(t-168)'],\n",
      "      dtype='object')\n",
      "range(-71, 1)\n",
      "Index(['value'], dtype='object')\n",
      "None\n",
      "Index(['working day', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5',\n",
      "       'week_6', 'week_7', 'fractional hour_sin', 'fractional hour_cos',\n",
      "       'day of year_sin', 'day of year_cos', 'value(t-168)'],\n",
      "      dtype='object')\n",
      "range(-71, 1)\n",
      "Index(['value'], dtype='object')\n",
      "None\n",
      "Index(['working day', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5',\n",
      "       'week_6', 'week_7', 'fractional hour_sin', 'fractional hour_cos',\n",
      "       'day of year_sin', 'day of year_cos', 'value(t-168)'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_input to have shape (72, 15) but got array with shape (72, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-650733cc1e46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m history = FULL_LSTMIMO.fit(global_inputs_X, global_inputs_T, batch_size=32, epochs=MAX_EPOCHS,\n\u001b[1;32m     38\u001b[0m                       \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                       callbacks=[early_stopping], verbose=1)\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mFD_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFULL_LSTMIMO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         steps=steps_per_epoch)\n\u001b[0m\u001b[1;32m    517\u001b[0m     (x, y, sample_weights,\n\u001b[1;32m    518\u001b[0m      \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py37/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    572\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    575\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected lstm_input to have shape (72, 15) but got array with shape (72, 1)"
     ]
    }
   ],
   "source": [
    "MAX_EPOCHS = 100\n",
    "BATCHSIZE = 32\n",
    "patience = 10\n",
    "HORIZON = 72\n",
    "\n",
    "\n",
    "FULL_LSTMIMO = tf.keras.models.Sequential([\n",
    "    # Shape [batch, time, features] => [batch, time, lstm_units]\n",
    "    tf.keras.layers.LSTM(32, input_shape=(HORIZON, 15)),\n",
    "    # Shape => [batch, time, features]\n",
    "    tf.keras.layers.Dense(HORIZON)\n",
    "])\n",
    "\n",
    "\n",
    "metrics = pd.DataFrame(columns=['mae','mape', 'rmse', 'B'], index=range(28))\n",
    "from progressbar import ProgressBar\n",
    "pbar = ProgressBar()\n",
    "dX_train = []\n",
    "dT_train = []\n",
    "dX_test = []\n",
    "dX_scaler = []\n",
    "for i,df in enumerate(datasets):\n",
    "    train_inputs, test_inputs, y_scaler = MIMO_fulldata_preparation(df, n_test=4380, T=72, HORIZON=72)\n",
    "    dX_train.append(train_inputs['X'])\n",
    "    dT_train.append(train_inputs['target'])\n",
    "    dX_test.append(test_inputs)\n",
    "    dX_scaler.append(y_scaler)\n",
    "global_inputs_X = tf.concat(dX_train, 0)\n",
    "global_inputs_T = tf.concat(dT_train, 0)\n",
    "#test_inputs = pd.concat(dn_test, axis=1)\n",
    "\n",
    "# full data LSTM MIMO compilation and fit\n",
    "FULL_LSTMIMO.compile(optimizer=tf.optimizers.Adam(), loss='mse', metrics=[tf.metrics.MeanSquaredError()])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min')\n",
    "        \n",
    "history = FULL_LSTMIMO.fit(global_inputs_X, global_inputs_T, batch_size=32, epochs=MAX_EPOCHS,\n",
    "                      validation_split=0.15,\n",
    "                      callbacks=[early_stopping], verbose=1)\n",
    "for i,df in enumerate(datasets):\n",
    "    FD_predictions = FULL_LSTMIMO.predict(dX_test[i-1]['X'])\n",
    "    FD_eval_df = create_evaluation_df(FD_predictions, dX_test[i-1], HORIZON, dX_scaler[i-1])\n",
    "    mae = validation(FD_eval_df['prediction'], FD_eval_df['actual'], 'MAE')\n",
    "    mape = validation(FD_eval_df['prediction'], FD_eval_df['actual'], 'MAPE')\n",
    "    rmse = validation(FD_eval_df['prediction'], FD_eval_df['actual'], 'RMSE')\n",
    "    #print('rmse {}'.format(rmse))\n",
    "    metrics.loc[i] = pd.Series({'mae':mae, 'mape':mape, 'rmse':rmse, 'B': names[i]})\n",
    "metrics.to_csv('./results/GEP/global/3days/fulldata_LSTM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-tower",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
